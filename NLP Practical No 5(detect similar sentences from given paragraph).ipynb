{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqW/GWGfLsK5AEiOK/f+Sv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Aim : Write  a python program to detect similar sentences from given paragraph."],"metadata":{"id":"aDgD6c5xayum"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpmnNkb_W_lQ","executionInfo":{"status":"ok","timestamp":1693553815750,"user_tz":-330,"elapsed":611,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"d67fbec2-e809-4014-b98c-be6f4ea38666"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":149}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"5ce3fp5jrTEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load the file containing sentences\n","def load_sentences(file_path):\n","   with open(file_path, 'r') as file:\n","    sentences = file.readlines()\n","   return [sentence.strip() for sentence in sentences]\n","\n","# Preprocess the input sentence\n","def preprocess_sentence(sentence):\n","   # Tokenize\n","   tokens = word_tokenize(sentence.lower())\n","\n","   # Remove stopwords\n","   stop_words = set(stopwords.words('english'))\n","   tokens = [token for token in tokens if token not in stop_words]\n","\n","   # Lemmatize\n","   lemmatizer = WordNetLemmatizer()\n","   tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","   return ' '.join(tokens)\n","\n","# Get the most similar sentence\n","def get_most_similar_sentence(user_input, sentences):\n","   # Preprocess input sentence\n","   preprocessed_user_input = preprocess_sentence(user_input)\n","\n","   # Preprocess sentences\n","   preprocessed_sentences = [preprocess_sentence(sentence) for sentence in\n","sentences]\n","\n","   # Create TF-IDF vectorizer\n","   vectorizer = TfidfVectorizer()\n","\n","   # Generate TF-IDF matrix\n","   tfidf_matrix = vectorizer.fit_transform([preprocessed_user_input] +\n","preprocessed_sentences)\n","\n","   # Calculate similarity scores\n","   similarity_scores = (tfidf_matrix * tfidf_matrix.T).A[0][1:]\n","\n","   # Find the index of the most similar sentence\n","   most_similar_index = similarity_scores.argmax()\n","   most_similar_sentence = sentences[most_similar_index]\n","\n","   return most_similar_sentence\n","\n","# Main program\n","def main():\n","   file_path = '/content/sentences.txt'  # Path to the file containing sentences\n","   sentences = load_sentences(file_path)\n","\n","   user_input = 'I love cooking.'\n","\n","   most_similar_sentence = get_most_similar_sentence(user_input, sentences)\n","   print('Most similar sentence:', most_similar_sentence)\n","\n","if __name__ == '__main__':\n","   main()"],"metadata":{"id":"l-k5WVzIhh53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(sentence):\n","    sentence = sentence.lower()\n","    tokens = word_tokenize(sentence)\n","    tokens = [word for word in tokens if word.isalnum()]\n","    tokens = [word for word in tokens if word not in stopwords.words('english')]\n","    return \" \".join(tokens)"],"metadata":{"id":"QArEPLcHkPAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_sentences = [preprocess(sentence) for sentence in sentences]"],"metadata":{"id":"vuUTLumsnZx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(processed_sentences)"],"metadata":{"id":"2pv5b3lGnlLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)"],"metadata":{"id":"z6sdPKChnxbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.9"],"metadata":{"id":"twso_kd4nzDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Similar Sentences:\")\n","for i in range(len(sentences)):\n","    for j in range(i+1, len(sentences)):\n","        if similarities[i][j] >= threshold:\n","            print(f\"Similarity: {similarities[i][j]:.2f}\")\n","            print(f\"Sentence {i+1}: {sentences[i]}\")\n","            print(f\"Sentence {j+1}: {sentences[j]}\")\n","            print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0EGrz67n0hI","executionInfo":{"status":"ok","timestamp":1693553344874,"user_tz":-330,"elapsed":4,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"532100aa-ae79-4f00-d3f7-af01ed875862"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similar Sentences:\n","Similarity: 1.00\n","Sentence 3: The aroma of freshly baked bread filled the air as I entered the bakery.\n","Sentence 11: The aroma of freshly baked bread filled the air as I entered the bakery.\n","\n","Similarity: 1.00\n","Sentence 9: The chef skillfully prepared a gourmet meal using locally sourced ingredients.\n","Sentence 12: The chef skillfully prepared a gourmet meal using locally sourced ingredients.\n","\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","import numpy as np\n","\n","#\n","texts=sentences\n","\n","# Vectorization of the texts\n","vectorizer = TfidfVectorizer(stop_words=\"english\")\n","X = vectorizer.fit_transform(texts)\n","# Used words (axis in our multi-dimensional space)\n","words = vectorizer.get_feature_names_out()\n","print(\"words\", words)\n","\n","n_clusters = 3\n","number_of_seeds_to_try = 10\n","max_iter = 300\n","\n","model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try).fit(X)\n","\n","labels = model.labels_\n","# Indices of preferable words in each cluster\n","ordered_words = model.cluster_centers_.argsort()[:, ::-1]\n","\n","print(\"centers:\", model.cluster_centers_)\n","print(\"labels\", labels)\n","print(\"inertia:\", model.inertia_)\n","\n","texts_per_cluster = np.zeros(n_clusters)\n","for i_cluster in range(n_clusters):\n","    for label in labels:\n","        if label == i_cluster:\n","            texts_per_cluster[i_cluster] += 1\n","\n","print(\"Top words per cluster:\")\n","for i_cluster in range(n_clusters):\n","    print(\"Cluster:\", i_cluster, \"texts:\", int(texts_per_cluster[i_cluster])),\n","    for term in ordered_words[i_cluster, :10]:\n","        print(\"\\t\" + words[term])\n","\n","print(\"\\n\")\n","print(\"Prediction\")\n","\n","text_to_predict = \"Why batman was defeated by superman so easily?\"\n","Y = vectorizer.transform([text_to_predict])\n","predicted_cluster = model.predict(Y)[0]\n","texts_per_cluster[predicted_cluster] += 1\n","\n","print(text_to_predict)\n","print(\"Cluster:\", predicted_cluster, \"texts:\", int(texts_per_cluster[predicted_cluster])),\n","for term in ordered_words[predicted_cluster, :10]:\n","    print(\"\\t\" + words[term])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjh5NSoNn18O","executionInfo":{"status":"ok","timestamp":1693553696905,"user_tz":-330,"elapsed":497,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"70174963-9338-4eb2-9cdf-078cda1c9ebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["words ['air' 'aroma' 'backyard' 'baked' 'bakery' 'believe' 'branches' 'bread'\n"," 'campfire' 'candle' 'cascaded' 'casting' 'cat' 'chef' 'clear' 'clover'\n"," 'crashed' 'creating' 'crystal' 'curious' 'depths' 'echoed' 'entered'\n"," 'filled' 'flame' 'flickering' 'forest' 'freshly' 'friends' 'gazed' 'glow'\n"," 'gourmet' 'hidden' 'horizon' 'ingredients' 'laughter' 'leaf' 'life'\n"," 'locally' 'meadow' 'meal' 'oak' 'old' 'planet' 'pool' 'pounced'\n"," 'prepared' 'reaching' 'rhythm' 'rocky' 'room' 'rose' 'shared' 'shore'\n"," 'skillfully' 'sky' 'soothing' 'sourced' 'stars' 'stood' 'stories' 'sun'\n"," 'tall' 'tree' 'using' 'warm' 'waterfall' 'waves' 'wise' 'wondering']\n","centers: [[0.         0.         0.0625     0.         0.         0.0625\n","  0.04166667 0.         0.04724556 0.05103104 0.04419417 0.04724556\n","  0.05103104 0.         0.04419417 0.0625     0.04724556 0.04724556\n","  0.04419417 0.05103104 0.04419417 0.04724556 0.         0.\n","  0.05103104 0.05103104 0.04419417 0.         0.04724556 0.0559017\n","  0.04724556 0.         0.04419417 0.04724556 0.         0.04724556\n","  0.0625     0.0559017  0.         0.04724556 0.         0.04166667\n","  0.04166667 0.0559017  0.04419417 0.05103104 0.         0.04166667\n","  0.04724556 0.04724556 0.04724556 0.04724556 0.04724556 0.04724556\n","  0.         0.04166667 0.04724556 0.         0.0559017  0.04166667\n","  0.04724556 0.04724556 0.04166667 0.04166667 0.         0.04724556\n","  0.04419417 0.04724556 0.04166667 0.0559017 ]\n"," [0.35355339 0.35355339 0.         0.35355339 0.35355339 0.\n","  0.         0.35355339 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.35355339 0.35355339\n","  0.         0.         0.         0.35355339 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.33333333 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.33333333 0.         0.         0.33333333 0.\n","  0.         0.         0.33333333 0.         0.33333333 0.\n","  0.         0.         0.         0.         0.33333333 0.\n","  0.         0.         0.         0.         0.         0.\n","  0.33333333 0.         0.         0.33333333 0.         0.\n","  0.         0.         0.         0.         0.33333333 0.\n","  0.         0.         0.         0.        ]]\n","labels [0 0 1 0 0 0 0 0 2 0 1 2]\n","inertia: 6.999999999999999\n","Top words per cluster:\n","Cluster: 0 texts: 8\n","\tbackyard\n","\tclover\n","\tbelieve\n","\tleaf\n","\tstars\n","\tlife\n","\tgazed\n","\twondering\n","\tplanet\n","\tflame\n","Cluster: 1 texts: 2\n","\tair\n","\taroma\n","\tbaked\n","\tbakery\n","\tentered\n","\tfilled\n","\tbread\n","\tfreshly\n","\tforest\n","\techoed\n","Cluster: 2 texts: 2\n","\tingredients\n","\tlocally\n","\tgourmet\n","\tchef\n","\tskillfully\n","\tsourced\n","\tmeal\n","\tusing\n","\tprepared\n","\tflickering\n","\n","\n","Prediction\n","Why batman was defeated by superman so easily?\n","Cluster: 0 texts: 9\n","\tbackyard\n","\tclover\n","\tbelieve\n","\tleaf\n","\tstars\n","\tlife\n","\tgazed\n","\twondering\n","\tplanet\n","\tflame\n"]}]},{"cell_type":"code","source":["pip install pyspark"],"metadata":{"id":"3961aj6Bp1f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693719052036,"user_tz":-330,"elapsed":51085,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"eb252ac8-7b16-4d6b-fb36-47c88db56bad"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=2d95646314cb5260afff67a084f93f841963da1c22be5d4c490f76d710aeeef1\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import numpy as np\n","from pyspark.ml import Pipeline\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F"],"metadata":{"id":"kIrhJFXfg-T7","executionInfo":{"status":"ok","timestamp":1693719066947,"user_tz":-330,"elapsed":1045,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["pip install spark-nlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDwfmKgVhvw2","executionInfo":{"status":"ok","timestamp":1693719142057,"user_tz":-330,"elapsed":6616,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"2d7f5671-a405-4236-df34-ca215b4d48fe"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting spark-nlp\n","  Downloading spark_nlp-5.1.0-py2.py3-none-any.whl (531 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.2/531.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: spark-nlp\n","Successfully installed spark-nlp-5.1.0\n"]}]},{"cell_type":"code","source":["import sparknlp\n","#from sparknlp.annotator import sparknlp.base\n","from sparknlp.pretrained import PretrainedPipeline\n","spark = sparknlp.start()\n","\n","# If you change the model, re-run all the cells below. # Applicable models: tfhub_use, tfhub_use_lg MODEL_NAME = \"tfhub_use\" os.environ['MODEL_NAME'] = MODEL_NAME\n","# To compare the similarity of sentences, enter them as strings in this list.\n","text_list = [\n","\"Sign up for our mailing list to get free offers and updates about our products!\",\n","\"It was raining, so I waited beneath the balcony outside the cafe.\",\n"," \"I stayed under the deck of the cafe because it was rainy outside.\",\n"," \"I like the cafe down the street because it's not too loud in there.\",\n","\"The coffee shop near where I live is quiet, so I like to go there.\",\n","\"Web traffic analysis shows that most Internet users browse on mobile nowadays.\",\n","\"The analytics show that modern web users mostly use their phone instead of their computers.\"\n","]"],"metadata":{"id":"gQQMpw3Qg-5w","executionInfo":{"status":"ok","timestamp":1693719250389,"user_tz":-330,"elapsed":53226,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.setInputCols(['document']) tokenizer.setOutputCol('token')\n","# Encodes the text as a single vector representing semantic features. sentence_encoder = UniversalSentenceEncoder.pretrained(name=MODEL_NAME) sentence_encoder.setInputCols(['document', 'token']) sentence_encoder.setOutputCol('sentence_embeddings')\n","nlp_pipeline = Pipeline(stages=[ document_assembler, tokenizer,\n","sentence_encoder\n","])\n","# Fit the model to an empty data frame so it can be used on inputs. empty_df = spark.createDataFrame([['']]).toDF('text') pipeline_model = nlp_pipeline.fit(empty_df)\n","light_pipeline = LightPipeline(pipeline_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"5BGL5DQOhf60","executionInfo":{"status":"error","timestamp":1693719255104,"user_tz":-330,"elapsed":369,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}},"outputId":"1cda6499-4ab5-4f27-b6c7-dd114b8ca500"},"execution_count":16,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-c63baf9b9986>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tokenizer.setInputCols(['document']) tokenizer.setOutputCol('token')\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["def get_similarity(input_list):\n","df = spark.createDataFrame(pd.DataFrame({'text': input_list})) result = light_pipeline.transform(df)\n","embeddings = []\n","for r in result.collect(): embeddings.append(r.sentence_embeddings[0].embeddings)\n","embeddings_matrix = np.array(embeddings)\n","return np.matmul(embeddings_matrix, embeddings_matrix.transpose())"],"metadata":{"id":"nuEK5DvViFdY","executionInfo":{"status":"aborted","timestamp":1693719250390,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shyam Nair","userId":"03988678318632858032"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LIV-NsCriLFv"},"execution_count":null,"outputs":[]}]}